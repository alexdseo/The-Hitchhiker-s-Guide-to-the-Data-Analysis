{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Project Workflow and Pipelines\n",
    "\n",
    "   Now, with defining your project's problem and setted goal, you need to set the project workflow and pipelines to make your project more efficient and reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most data-science projects have the same set of tasks:\n",
    "\n",
    "ETL: extracting data from its source, transforming it, then loading it into a database. Remember, ETL stands for Extract , Transform and Load.\n",
    "Pre-process data: This might include imputing missing values and choosing the training and testing sets.\n",
    "Create features: Recombine and enrich the data to create features aiding the modelling work.\n",
    "Train the model(s): You can try different algorithms, features, and so on.\n",
    "Assess performance on the test set: Using an appropriate metric (e.g. $Precision@k$, $Recall$, $AUC$), examine the performance of your model \"out of sample.\"\n",
    "Think of new things to try. Repeat steps 1 through 4 as appropriate.\n",
    "If the code base is not structured and named well, you might be struggling to remember the details of each step once you have built a few models. What features did you use for each? What training and testing split? What hyperparameters?\n",
    "\n",
    "Your code might be getting messy too. Did you overwrite the code for the previous model? Maybe you copied, pasted, and edited code from an earlier model. Can you still read what's there? It can quickly become a hodgepodge that requires heroics to decipher.\n",
    "\n",
    "In this session, we will introduce a workflow that can avoid (or at least reduce) these problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is helpful to structure the data into multiple layers. In data bases, a layer is expressed as schema. In most other formats, they are expressed through a directory structure.\n",
    "\n",
    "Raw\n",
    "The data we receive from the partners and external sources is the raw data. Raw data is immutable. Quoting from the popular workflow package Data Science Cookiecutter:\n",
    "\n",
    "Don't ever edit your raw data, especially not manually, and especially not in Excel. Don't overwrite your raw data. Don't save multiple versions of the raw data. Treat the data (and its format) as immutable. The code you write should move the raw data through a pipeline to your final analysis. You shouldn't have to run all of the steps every time you want to make a new figure (see Analysis is a DAG), but anyone should be able to reproduce the final products with only the code in src and the data in data/raw.\n",
    "\n",
    "Intermediate\n",
    "If the raw data is messy, it is advisable to create an intermediate layer that consists of tidy copies of the raw data. Typical situations where this is useful are:\n",
    "\n",
    "Data is received in multiple different file types\n",
    "Data fields are not typed (e.g. csv files, excel) or poorly typed (dates as strings, inconsistent date formats)\n",
    "Column names are unclear, have spaces, special characters, or there are no column names\n",
    "The transformations from raw to intermediate should be limited to fix the issues mentioned above. We should not combine different data sets or create calculated fields. This is reserved for the next layer.\n",
    "\n",
    "Typical storage formats for the intermediate layer are a data base (e.g. postgres) or parquet files.\n",
    "\n",
    "Processed\n",
    "To perform the modelling work, the input data needs to be combined and enriched, for example by creating features. The data sets that are created in this process are stored in the processed layer. Sometimes it can be useful to split this layer out into a domain data model, a feature layer and a master layer but the exact layering will depend on the project context.\n",
    "\n",
    "Models\n",
    "The processed data is used to train predictive models, explanatory models, recommender engines and optimisation algorithms. The trained models are stored in the model layer. In contrast to the previous layers, models are usually stored in pickle because they are not in tabular format.\n",
    "\n",
    "Model output\n",
    "Model performance metrics, model selection information and predictions are kept in the model output layer.\n",
    "\n",
    "Reporting\n",
    "Reporting can be performed across the pipeline. For example, there might be data quality reports on the inputs, distribution analysis on the processed data, predictions, explanations, recommendations that are provided to the user, and performance evaluation and tracking. If a front-end is constructed, it will access the reporting layer to display information to the users and developers. For example, a Tableau dashboard, power BI, a jupyter notebook or an excel output will read from the reporting layer. Accordingly, the format of the data in the reporting layer will be adjusted to the front end of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Software setup, YAML, License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
